# Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ
## Ø§Ù†ÙˆØ§Ø¹Ù‡
- *Linear Regression* â€“ Models the relationship as a straight line (e.g., predicting house prices based on square footage).
- *Multiple Regression* â€“ Extends linear regression by using multiple predictors.
- *Polynomial Regression* â€“ Uses polynomial functions for nonlinear relationships.
- *Logistic Regression* â€“ Used for classification problems, not actual regression.
- *Regularized Regression* â€“ Includes Ridge (L2) and Lasso (L1) regression to prevent overfitting.



```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

```


### Ù…ÙƒØªØ¨Ø§Øª Ø§Ø®Ø±Ù‰
<details>
  

 
```

Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø§ÙŠØ«ÙˆÙ† ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
1. Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ· (Linear Regression)
from sklearn.linear_model import LinearRegression
model = LinearRegression()


2. Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ (Multiple Linear Regression)
from sklearn.linear_model import LinearRegression
model = LinearRegression()

3. Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…Ù†ØªØ¸Ù… (Regularized Regression)
Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…Ù†ØªØ¸Ù… ÙŠØ¶ÙŠÙ Ø¹Ù‚ÙˆØ¨Ø© (penalty) Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ù†Ø¹ Ø§Ù„ØªØ­Ø§ÙŠÙ„ (overfitting). ØªØ´Ù…Ù„ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ©:

from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0)  # alpha Ù‡Ùˆ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø©


from sklearn.linear_model import Lasso
model = Lasso(alpha=0.1)

ğŸ”¹ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± ElasticNet
Ø§Ù„Ù‡Ø¯Ù: ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø¹Ù‚ÙˆØ¨Ø§Øª L1 Ùˆ L2. Ù…ÙÙŠØ¯ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù„Ø¯ÙŠÙƒ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø©.
from sklearn.linear_model import ElasticNet
model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio ÙŠØ­Ø¯Ø¯ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© L1


from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

from sklearn.svm import SVR
model = SVR(kernel='linear')  # ÙŠÙ…ÙƒÙ† Ø§Ø®ØªÙŠØ§Ø± kernel Ù…Ø®ØªÙ„Ù Ù…Ø«Ù„ 'rbf'

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100)

from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor(n_neighbors=3)

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=100)

from xgboost import XGBRegressor
model = XGBRegressor(n_estimators=100)

from sklearn.ensemble import HistGradientBoostingRegressor
model = HistGradientBoostingRegressor()

from sklearn.neural_network import MLPRegressor
model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000)

```
</details>


```python
# Simulated dataset: House sizes (sqft) and corresponding prices ($1000s)
np.random.seed(42)
house_size = np.random.randint(500, 4000, 50)  # House size in square feet
house_price = 50 + 4.5 * house_size + np.random.normal(0, 300, 50)  # Price with some noise

# Convert to DataFrame
df = pd.DataFrame({'Size': house_size, 'Price': house_price})

# Split data into training and testing sets
X = df[['Size']]  # Independent variable (feature)
y = df['Price']   # Dependent variable (target)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

```


```python
df.size
```




    100




```python
X_test
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13</th>
      <td>3944</td>
    </tr>
    <tr>
      <th>39</th>
      <td>3247</td>
    </tr>
    <tr>
      <th>30</th>
      <td>3885</td>
    </tr>
    <tr>
      <th>45</th>
      <td>3505</td>
    </tr>
    <tr>
      <th>17</th>
      <td>2185</td>
    </tr>
    <tr>
      <th>48</th>
      <td>1767</td>
    </tr>
    <tr>
      <th>26</th>
      <td>1455</td>
    </tr>
    <tr>
      <th>25</th>
      <td>1715</td>
    </tr>
    <tr>
      <th>32</th>
      <td>2800</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1269</td>
    </tr>
  </tbody>
</table>
</div>




```python
y_test
```




    13    17586.663965
    39    14864.830223
    30    17079.512815
    45    16171.930594
    17     9902.497184
    48     8365.151703
    26     6676.545812
    25     8564.303027
    32    12396.974023
    19     6120.767832
    Name: Price, dtype: float64




```python
# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Get the model parameters
intercept = model.intercept_
slope = model.coef_[0]

print(f"Regression Equation: Price = {intercept:.2f} + {slope:.2f} * Size")

```

    Regression Equation: Price = 39.52 + 4.52 * Size



```python
# Predict house prices
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared (RÂ²): {r2:.2f}")

```

    Mean Squared Error: 138043.26
    R-squared (RÂ²): 0.99



```python
plt.scatter(X_test, y_test, color='blue', label="Actual Prices")
plt.plot(X_test, y_pred, color='red', linewidth=2, label="Regression Line")
plt.xlabel("House Size (sqft)")
plt.ylabel("Price ($1000s)")
plt.title("Simple Linear Regression: House Price Prediction")
plt.legend()
plt.show()

```


    
![png](output_10_0.png)
    



```python
from sklearn.ensemble import RandomForestRegressor
X = [[1], [2], [3], [4], [5]]
y = [1, 2, 3, 4, 5]
model = RandomForestRegressor(n_estimators=100)
model.fit(X, y)
predictions = model.predict([[3]])
print(predictions)

```

    [2.71]


### Ù…Ø«Ø§Ù„ Ø¨Ø±Ù…Ø¬ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±



```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X = np.array([[1], [2], [3], [4], [5]])  # Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©
y = np.array([30100, 35300, 40020, 45000, 50200])  # Ø§Ù„Ø±ÙˆØ§ØªØ¨

# Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = LinearRegression()
model.fit(X, y)

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
beta_0 = model.intercept_  # Ø§Ù„ØªÙ‚Ø§Ø·Ø¹
beta_1 = model.coef_[0]  # Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±

# Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ…
y_pred = model.predict(X)

# Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
plt.scatter(X, y, color='blue', label='Real Data')
plt.plot(X, y_pred, color='red', label='Linear Predict')
plt.xlabel('Years')
plt.ylabel('Salary')
plt.legend()
plt.show()

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
print(f"Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Î²0): {beta_0}")
print(f"Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± (Î²1): {beta_1}")
#chatGBT Example
```


    
![png](output_13_0.png)
    


    Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Î²0): 25154.000000000004
    Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± (Î²1): 4989.999999999999



```python
# Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X = np.array([[1, 3], [2, 4], [3, 5], [4, 6], [5, 7]])  # Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ…
y = np.array([30000, 35000, 40000, 45000, 50000])  # Ø§Ù„Ø±ÙˆØ§ØªØ¨

# Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = LinearRegression()
model.fit(X, y)

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
beta_0 = model.intercept_  # Ø§Ù„ØªÙ‚Ø§Ø·Ø¹
beta_1, beta_2 = model.coef_  # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±

# Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ…
y_pred = model.predict(X)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
print(f"Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Î²0): {beta_0}")
print(f"Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ø¹Ø¯Ø¯ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø© (Î²1): {beta_1}")
print(f"Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… (Î²2): {beta_2}")

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
print(f"Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {y_pred}")
# ÙÙ‡Ù… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# Ø­Ø³Ø§Ø¨ R^2
r_squared = model.score(X, y)
print(f"Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ¯ (R^2): {r_squared}")


```

    Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Î²0): 20000.0
    Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ø¹Ø¯Ø¯ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø© (Î²1): 2500.0000000000005
    Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… (Î²2): 2500.0
    Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: [30000. 35000. 40000. 45000. 50000.]
    Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ¯ (R^2): 1.0



```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Ø­Ø³Ø§Ø¨ MAE
mae = mean_absolute_error(y, y_pred)
print(f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ù…Ø·Ù„Ù‚ (MAE): {mae}")

```

    Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ù…Ø·Ù„Ù‚ (MAE): 0.0



```python
# Ø­Ø³Ø§Ø¨ MSE
mse = mean_squared_error(y, y_pred)
print(f"Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø®Ø·Ø£ (MSE): {mse}")

```

    Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø®Ø·Ø£ (MSE): 0.0



```python
# Ø­Ø³Ø§Ø¨ RMSE
rmse = np.sqrt(mse)
print(f"Ø¬Ø°Ø± Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø®Ø·Ø£ (RMSE): {rmse}")

```

    Ø¬Ø°Ø± Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø®Ø·Ø£ (RMSE): 0.0



```python
from sklearn.linear_model import LinearRegression
import numpy as np

# Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹ÙŠÙ†Ø©: Ù…ØªØºÙŠØ±ÙŠÙ† Ù…Ø³ØªÙ‚Ù„ÙŠÙ† (x1, x2) ÙˆÙ…ØªØºÙŠØ± ØªØ§Ø¨Ø¹ (y)
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
y = np.array([5, 7, 9, 11, 13])  # Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„ØªØ§Ø¨Ø¹

# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±
model = LinearRegression()

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model.fit(X, y)

# Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
print(f"Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø§Ø¨Øª (Î²0): {model.intercept_}")
print(f"Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Î²1, Î²2): {model.coef_}")

# Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ…
y_pred = model.predict(X)
print(f"Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {y_pred}")

```

    Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø§Ø¨Øª (Î²0): 2.0
    Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Î²1, Î²2): [1. 1.]
    Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: [ 5.  7.  9. 11. 13.]


## Ridge and Lasso.


```python
import numpy as np
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error

# Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø©: Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø© (X) ÙˆØ§Ù„Ù…ØªØºÙŠØ± Ø§Ù„ØªØ§Ø¨Ø¹ (y)
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3  # y = 1*x1 + 2*x2 + 3

# Ø§Ù†Ø­Ø¯Ø§Ø± Ridge
ridge_model = Ridge(alpha=1.0)  # alpha Ù‡Ùˆ Ù…Ø¹Ù„Ù…Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ… (Î»)
ridge_model.fit(X, y)
y_pred_ridge = ridge_model.predict(X)
print("Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ridge:", ridge_model.coef_)
print("Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø§Ø¨Øª Ù„Ù€ Ridge:", ridge_model.intercept_)
print("MSE Ù„Ù€ Ridge:", mean_squared_error(y, y_pred_ridge))

# Ø§Ù†Ø­Ø¯Ø§Ø± Lasso
lasso_model = Lasso(alpha=0.5)  # alpha Ù‡Ùˆ Ù…Ø¹Ù„Ù…Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ… (Î»)
lasso_model.fit(X, y)
y_pred_lasso = lasso_model.predict(X)
print("Ù…Ø¹Ø§Ù…Ù„Ø§Øª Lasso:", lasso_model.coef_)
print("Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø§Ø¨Øª Ù„Ù€ Lasso:", lasso_model.intercept_)
print("MSE Ù„Ù€ Lasso:", mean_squared_error(y, y_pred_lasso))

```

    Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ridge: [0.8 1.4]
    Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø§Ø¨Øª Ù„Ù€ Ridge: 4.5
    MSE Ù„Ù€ Ridge: 0.24999999999999956
    Ù…Ø¹Ø§Ù…Ù„Ø§Øª Lasso: [0.  1.5]
    Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø§Ø¨Øª Ù„Ù€ Lasso: 5.5
    MSE Ù„Ù€ Lasso: 0.625


Ø§Ù„ØªÙ†Ø¸ÙŠÙ… ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ Ù…Ù†Ø¹ Ø§Ù„ØªÙÙˆÙ‚ ÙÙŠ Ø§Ù„ØªÙƒÙŠÙ Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ‚Ù„ÙŠØµ ØªØ£Ø«ÙŠØ± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©.

Ø§Ù†Ø­Ø¯Ø§Ø± Ridge (L2) ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªÙ‚Ù„ÙŠØµ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù†Ø­Ùˆ Ø§Ù„ØµÙØ± Ø¯ÙˆÙ† Ø¬Ø¹Ù„Ù‡Ø§ ØµÙØ±Ù‹Ø§ ØªÙ…Ø§Ù…Ù‹Ø§.

Ø§Ù†Ø­Ø¯Ø§Ø± Lasso (L1) ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ¬Ø¹Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ØªØ³Ø§ÙˆÙŠ ØµÙØ±Ù‹Ø§ØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª.


